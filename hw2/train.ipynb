{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujh1123/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>utterance</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356</td>\n",
       "      <td>6</td>\n",
       "      <td>Start by reading the preliminary information</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>357</td>\n",
       "      <td>6</td>\n",
       "      <td>\"Hello Ms. Klein, I am responsible for you in ...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>358</td>\n",
       "      <td>6</td>\n",
       "      <td>Since yesterday afternoon?</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>359</td>\n",
       "      <td>6</td>\n",
       "      <td>Hmm, but the shortness of breath only came whi...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360</td>\n",
       "      <td>6</td>\n",
       "      <td>Have you ever laid down during the day when yo...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1526</td>\n",
       "      <td>23</td>\n",
       "      <td>Is this the right career for you? Do you have ...</td>\n",
       "      <td>SF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1527</td>\n",
       "      <td>23</td>\n",
       "      <td>Your nightmares aren't about your work either?...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1528</td>\n",
       "      <td>23</td>\n",
       "      <td>But that started at some point? So it's been m...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1529</td>\n",
       "      <td>23</td>\n",
       "      <td>Are there any illnesses in your family?</td>\n",
       "      <td>SF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1530</td>\n",
       "      <td>23</td>\n",
       "      <td>Do you feel like you're suffocating? Is the ac...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1850 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  source                                          utterance classes\n",
       "0     356       6       Start by reading the preliminary information   OTHER\n",
       "1     357       6  \"Hello Ms. Klein, I am responsible for you in ...   OTHER\n",
       "2     358       6                         Since yesterday afternoon?   OTHER\n",
       "3     359       6  Hmm, but the shortness of breath only came whi...   OTHER\n",
       "4     360       6  Have you ever laid down during the day when yo...   OTHER\n",
       "..    ...     ...                                                ...     ...\n",
       "402  1526      23  Is this the right career for you? Do you have ...      SF\n",
       "403  1527      23  Your nightmares aren't about your work either?...   OTHER\n",
       "404  1528      23  But that started at some point? So it's been m...   OTHER\n",
       "405  1529      23            Are there any illnesses in your family?      SF\n",
       "406  1530      23  Do you feel like you're suffocating? Is the ac...   OTHER\n",
       "\n",
       "[1850 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('dataset/train.tsv', sep='\\t')\n",
    "val_data = pd.read_csv('dataset/val.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('dataset/test.tsv', sep='\\t')\n",
    "train_data = pd.concat([train_data, val_data], axis=0)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujh1123/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_classes = 6 \n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tag_list=[\"AM\",\"MS\",\"OTHER\",\"PH\",\"SF\",\"SR\"]\n",
    "tag_to_idx={}\n",
    "for i in range(len(tag_list)):\n",
    "    tag_to_idx[tag_list[i]]=i\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utterance = str(self.data.iloc[idx]['utterance'])\n",
    "        labels = self.data.iloc[idx]['classes'] \n",
    "        labels = self.label_encoded(labels)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            utterance,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "    def label_encoded(self,labels):\n",
    "        encode_label = np.zeros(6,dtype=int)\n",
    "        list_label = labels.split(\",\")\n",
    "        for i in list_label:\n",
    "            idx = tag_to_idx[i]\n",
    "            encode_label[idx] = 1\n",
    "        return encode_label \n",
    "\n",
    "max_len = 256\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = CustomDataset(train_data, tokenizer, max_len)\n",
    "val_dataset = CustomDataset(val_data, tokenizer, max_len)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW \n",
    "def init_model(model,epochs):\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-6,betas=(0.9, 0.999))\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=500,num_training_steps=total_steps)\n",
    "    return model , optimizer , scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1,1,0.2,1,1,1]\n",
    "weights = torch.tensor(weights).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "def evalute(dataloader,model):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_target = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(dataloader):\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_attn_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            y_pred.extend(torch.sigmoid(logits.logits).cpu().detach().numpy().tolist())         \n",
    "            y_target.extend(b_labels.cpu().detach().numpy().tolist())\n",
    "    y_preds = (np.array(y_pred)>0.5).astype(int)\n",
    "    marco_f1= f1_score(y_target,y_preds,average='macro')\n",
    "    # print(\"marco f1 score : \",marco_f1)\n",
    "    return marco_f1\n",
    "def train(model,train_dataloader,val_dataloader,optimizer,scheduler,path,epochs,evaluation):\n",
    "    max_score = 0\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'F1 score':^9} | {'Elapsed':^9}\")\n",
    "    for epoch_i in range(epochs):\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attn_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()    \n",
    "            model.zero_grad()\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        if evaluation == True:\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            score = evalute(val_dataloader,model)\n",
    "            if score > max_score:\n",
    "                torch.save(model.state_dict(),path)\n",
    "                # print('save model')\n",
    "                max_score = score\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {score:^9.6f} | {time_elapsed:^9.2f}\")\n",
    "        print(\"\")\n",
    "    print(\"best score: \",max_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.455426   | 0.359184  |   25.30  \n",
      "\n",
      "   2    |   0.267134   | 0.579066  |   25.12  \n",
      "\n",
      "   3    |   0.186373   | 0.802905  |   25.12  \n",
      "\n",
      "   4    |   0.139381   | 0.857821  |   24.91  \n",
      "\n",
      "   5    |   0.112082   | 0.893845  |   24.87  \n",
      "\n",
      "   6    |   0.082744   | 0.929405  |   24.99  \n",
      "\n",
      "   7    |   0.067112   | 0.950980  |   24.99  \n",
      "\n",
      "   8    |   0.056745   | 0.958547  |   25.04  \n",
      "\n",
      "   9    |   0.050904   | 0.959204  |   25.02  \n",
      "\n",
      "  10    |   0.047139   | 0.963462  |   25.09  \n",
      "\n",
      "best score:  0.9634623000657211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.445409   | 0.153328  |   25.08  \n",
      "\n",
      "   2    |   0.280010   | 0.697994  |   25.08  \n",
      "\n",
      "   3    |   0.190338   | 0.810380  |   25.08  \n",
      "\n",
      "   4    |   0.139049   | 0.869351  |   25.05  \n",
      "\n",
      "   5    |   0.101937   | 0.896669  |   24.66  \n",
      "\n",
      "   6    |   0.080233   | 0.945969  |   25.07  \n",
      "\n",
      "   7    |   0.064078   | 0.962625  |   25.04  \n",
      "\n",
      "   8    |   0.055440   | 0.962510  |   25.11  \n",
      "\n",
      "   9    |   0.050203   | 0.964070  |   25.02  \n",
      "\n",
      "  10    |   0.045236   | 0.961114  |   24.39  \n",
      "\n",
      "best score:  0.9640699658605593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.443088   | 0.123077  |   25.00  \n",
      "\n",
      "   2    |   0.279471   | 0.673096  |   25.05  \n",
      "\n",
      "   3    |   0.190385   | 0.781777  |   24.97  \n",
      "\n",
      "   4    |   0.138291   | 0.848288  |   25.00  \n",
      "\n",
      "   5    |   0.105600   | 0.896505  |   25.03  \n",
      "\n",
      "   6    |   0.079883   | 0.934363  |   25.07  \n",
      "\n",
      "   7    |   0.066515   | 0.949486  |   25.06  \n",
      "\n",
      "   8    |   0.055982   | 0.956073  |   25.04  \n",
      "\n",
      "   9    |   0.048644   | 0.965029  |   25.05  \n",
      "\n",
      "  10    |   0.046596   | 0.963675  |   24.94  \n",
      "\n",
      "best score:  0.9650287425240235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.457994   | 0.124066  |   24.81  \n",
      "\n",
      "   2    |   0.298475   | 0.675603  |   25.03  \n",
      "\n",
      "   3    |   0.196961   | 0.835213  |   25.01  \n",
      "\n",
      "   4    |   0.136942   | 0.888423  |   25.05  \n",
      "\n",
      "   5    |   0.102976   | 0.912664  |   25.05  \n",
      "\n",
      "   6    |   0.079759   | 0.929790  |   25.06  \n",
      "\n",
      "   7    |   0.064073   | 0.940005  |   25.01  \n",
      "\n",
      "   8    |   0.055600   | 0.947066  |   25.00  \n",
      "\n",
      "   9    |   0.047919   | 0.954752  |   25.02  \n",
      "\n",
      "  10    |   0.044308   | 0.956464  |   25.03  \n",
      "\n",
      "best score:  0.9564637251926119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.438998   | 0.196548  |   24.99  \n",
      "\n",
      "   2    |   0.277621   | 0.619761  |   25.06  \n",
      "\n",
      "   3    |   0.192805   | 0.790778  |   25.00  \n",
      "\n",
      "   4    |   0.145628   | 0.825238  |   24.86  \n",
      "\n",
      "   5    |   0.114507   | 0.889540  |   25.01  \n",
      "\n",
      "   6    |   0.089674   | 0.921067  |   25.04  \n",
      "\n",
      "   7    |   0.072389   | 0.944456  |   25.01  \n",
      "\n",
      "   8    |   0.060238   | 0.955345  |   25.08  \n",
      "\n",
      "   9    |   0.053861   | 0.963382  |   25.07  \n",
      "\n",
      "  10    |   0.050157   | 0.963382  |   25.17  \n",
      "\n",
      "best score:  0.9633815040016871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.445202   | 0.121274  |   24.90  \n",
      "\n",
      "   2    |   0.292268   | 0.598174  |   25.09  \n",
      "\n",
      "   3    |   0.198114   | 0.771699  |   24.64  \n",
      "\n",
      "   4    |   0.142934   | 0.852276  |   25.08  \n",
      "\n",
      "   5    |   0.105097   | 0.888667  |   25.13  \n",
      "\n",
      "   6    |   0.086121   | 0.931431  |   25.17  \n",
      "\n",
      "   7    |   0.067958   | 0.949588  |   25.11  \n",
      "\n",
      "   8    |   0.057961   | 0.955989  |   25.06  \n",
      "\n",
      "   9    |   0.050002   | 0.956051  |   25.08  \n",
      "\n",
      "  10    |   0.048514   | 0.956860  |   25.13  \n",
      "\n",
      "best score:  0.9568600361168675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.450384   | 0.120198  |   24.87  \n",
      "\n",
      "   2    |   0.288392   | 0.658644  |   24.21  \n",
      "\n",
      "   3    |   0.192405   | 0.792902  |   25.01  \n",
      "\n",
      "   4    |   0.140690   | 0.847920  |   25.01  \n",
      "\n",
      "   5    |   0.111932   | 0.883597  |   24.92  \n",
      "\n",
      "   6    |   0.085108   | 0.914420  |   24.76  \n",
      "\n",
      "   7    |   0.069786   | 0.937754  |   24.90  \n",
      "\n",
      "   8    |   0.061098   | 0.951267  |   24.98  \n",
      "\n",
      "   9    |   0.053326   | 0.959681  |   24.66  \n",
      "\n",
      "  10    |   0.050003   | 0.959681  |   24.60  \n",
      "\n",
      "best score:  0.9596813182443104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.457152   | 0.116366  |   25.03  \n",
      "\n",
      "   2    |   0.291832   | 0.660338  |   25.04  \n",
      "\n",
      "   3    |   0.193489   | 0.779600  |   24.89  \n",
      "\n",
      "   4    |   0.140483   | 0.823199  |   25.03  \n",
      "\n",
      "   5    |   0.103745   | 0.902313  |   25.02  \n",
      "\n",
      "   6    |   0.080398   | 0.929390  |   25.03  \n",
      "\n",
      "   7    |   0.066071   | 0.954399  |   25.00  \n",
      "\n",
      "   8    |   0.053470   | 0.959024  |   25.03  \n",
      "\n",
      "   9    |   0.048846   | 0.966702  |   25.04  \n",
      "\n",
      "  10    |   0.044158   | 0.968325  |   25.05  \n",
      "\n",
      "best score:  0.9683248182768422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.450990   | 0.123667  |   25.11  \n",
      "\n",
      "   2    |   0.324525   | 0.448498  |   25.14  \n",
      "\n",
      "   3    |   0.224309   | 0.758295  |   25.08  \n",
      "\n",
      "   4    |   0.163174   | 0.822042  |   25.01  \n",
      "\n",
      "   5    |   0.123893   | 0.860791  |   25.10  \n",
      "\n",
      "   6    |   0.097112   | 0.899014  |   25.00  \n",
      "\n",
      "   7    |   0.081784   | 0.930565  |   25.08  \n",
      "\n",
      "   8    |   0.066890   | 0.945297  |   25.13  \n",
      "\n",
      "   9    |   0.058132   | 0.949810  |   25.04  \n",
      "\n",
      "  10    |   0.056099   | 0.951750  |   25.02  \n",
      "\n",
      "best score:  0.9517497704316634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.452114   | 0.155369  |   25.01  \n",
      "\n",
      "   2    |   0.282009   | 0.680537  |   25.07  \n",
      "\n",
      "   3    |   0.194315   | 0.802372  |   25.03  \n",
      "\n",
      "   4    |   0.137307   | 0.836478  |   24.67  \n",
      "\n",
      "   5    |   0.107195   | 0.910047  |   25.07  \n",
      "\n",
      "   6    |   0.083758   | 0.933651  |   25.07  \n",
      "\n",
      "   7    |   0.066623   | 0.945339  |   25.10  \n",
      "\n",
      "   8    |   0.057326   | 0.961374  |   25.12  \n",
      "\n",
      "   9    |   0.050581   | 0.964471  |   25.11  \n",
      "\n",
      "  10    |   0.047286   | 0.965493  |   24.84  \n",
      "\n",
      "best score:  0.9654931742772148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.456252   | 0.094595  |   24.70  \n",
      "\n",
      "   2    |   0.310408   | 0.574401  |   25.10  \n",
      "\n",
      "   3    |   0.221612   | 0.742196  |   25.10  \n",
      "\n",
      "   4    |   0.168353   | 0.819720  |   24.91  \n",
      "\n",
      "   5    |   0.132952   | 0.845227  |   24.29  \n",
      "\n",
      "   6    |   0.104168   | 0.893921  |   24.97  \n",
      "\n",
      "   7    |   0.081875   | 0.909027  |   25.04  \n",
      "\n",
      "   8    |   0.068690   | 0.931946  |   24.92  \n",
      "\n",
      "   9    |   0.060670   | 0.931568  |   25.08  \n",
      "\n",
      "  10    |   0.057032   | 0.931643  |   24.87  \n",
      "\n",
      "best score:  0.9319456393143078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.447816   | 0.114996  |   25.09  \n",
      "\n",
      "   2    |   0.296782   | 0.551500  |   25.07  \n",
      "\n",
      "   3    |   0.202058   | 0.769935  |   25.10  \n",
      "\n",
      "   4    |   0.156782   | 0.834250  |   25.08  \n",
      "\n",
      "   5    |   0.117233   | 0.888918  |   25.05  \n",
      "\n",
      "   6    |   0.094090   | 0.897027  |   25.10  \n",
      "\n",
      "   7    |   0.078933   | 0.920986  |   24.93  \n",
      "\n",
      "   8    |   0.066387   | 0.936963  |   24.94  \n",
      "\n",
      "   9    |   0.059546   | 0.949267  |   25.11  \n",
      "\n",
      "  10    |   0.056836   | 0.947685  |   25.09  \n",
      "\n",
      "best score:  0.949267198664789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.454921   | 0.115717  |   25.11  \n",
      "\n",
      "   2    |   0.290890   | 0.636284  |   25.03  \n",
      "\n",
      "   3    |   0.196743   | 0.775691  |   25.08  \n",
      "\n",
      "   4    |   0.153940   | 0.840033  |   25.09  \n",
      "\n",
      "   5    |   0.120065   | 0.860979  |   25.05  \n",
      "\n",
      "   6    |   0.091093   | 0.906707  |   24.98  \n",
      "\n",
      "   7    |   0.069914   | 0.941420  |   24.99  \n",
      "\n",
      "   8    |   0.060896   | 0.952266  |   25.03  \n",
      "\n",
      "   9    |   0.052625   | 0.955804  |   24.78  \n",
      "\n",
      "  10    |   0.049422   | 0.957530  |   24.70  \n",
      "\n",
      "best score:  0.9575303385887938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.458347   | 0.120459  |   24.97  \n",
      "\n",
      "   2    |   0.289441   | 0.528455  |   24.89  \n",
      "\n",
      "   3    |   0.210908   | 0.774943  |   25.11  \n",
      "\n",
      "   4    |   0.162373   | 0.845530  |   25.13  \n",
      "\n",
      "   5    |   0.119272   | 0.881308  |   25.12  \n",
      "\n",
      "   6    |   0.091046   | 0.906263  |   25.14  \n",
      "\n",
      "   7    |   0.078660   | 0.940096  |   25.07  \n",
      "\n",
      "   8    |   0.065308   | 0.947408  |   25.14  \n",
      "\n",
      "   9    |   0.056325   | 0.951958  |   25.13  \n",
      "\n",
      "  10    |   0.052300   | 0.951958  |   25.00  \n",
      "\n",
      "best score:  0.9519579009409477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  |  Train Loss  | F1 score  |  Elapsed \n",
      "   1    |   0.450514   | 0.174143  |   25.03  \n",
      "\n",
      "   2    |   0.298316   | 0.509090  |   25.03  \n",
      "\n",
      "   3    |   0.203539   | 0.744770  |   24.80  \n",
      "\n",
      "   4    |   0.159381   | 0.822267  |   25.21  \n",
      "\n",
      "   5    |   0.120653   | 0.868048  |   25.14  \n",
      "\n",
      "   6    |   0.095058   | 0.915715  |   25.14  \n",
      "\n",
      "   7    |   0.077401   | 0.927647  |   25.17  \n",
      "\n",
      "   8    |   0.067333   | 0.940945  |   25.06  \n",
      "\n",
      "   9    |   0.060306   | 0.944802  |   25.07  \n",
      "\n",
      "  10    |   0.054952   | 0.944802  |   25.08  \n",
      "\n",
      "best score:  0.9448023213641658\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "for i in range(15):\n",
    "    epochs = 10\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=num_classes)\n",
    "    bert_classifier, optimizer, scheduler = init_model(model,epochs=epochs)\n",
    "    path = f\"w_weight{i}.pth\"\n",
    "    train(bert_classifier, train_dataloader, val_dataloader,optimizer,scheduler ,path,epochs=epochs, evaluation=True)\n",
    "    model_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wujh1123/NLP/hw2/train.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu3.cplab.cs.nycu.edu.tw/home/wujh1123/NLP/hw2/train.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# print(\"marco f1 score : \",marco_f1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu3.cplab.cs.nycu.edu.tw/home/wujh1123/NLP/hw2/train.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m marco_f1\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu3.cplab.cs.nycu.edu.tw/home/wujh1123/NLP/hw2/train.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(essem_evalute(val_dataloader,model_list))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_list' is not defined"
     ]
    }
   ],
   "source": [
    "def essem_evalute(dataloader,model_list):\n",
    "    for i in range(len(model_list)):\n",
    "        model_list[i].eval()\n",
    "    y_pred = []\n",
    "    y_target = []\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_attn_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            tmp = 0\n",
    "            for i in range(len(model_list)):\n",
    "                logits = model_list[i](b_input_ids, b_attn_mask)\n",
    "                tmp += torch.sigmoid(logits.logits)\n",
    "            tmp = tmp / len(model_list)\n",
    "            y_pred.extend(tmp.cpu().detach().numpy().tolist())         \n",
    "            y_target.extend(b_labels.cpu().detach().numpy().tolist())\n",
    "    y_preds = (np.array(y_pred)>0.5).astype(int)\n",
    "    marco_f1= f1_score(y_target,y_preds,average='macro')\n",
    "    # print(\"marco f1 score : \",marco_f1)\n",
    "    return marco_f1\n",
    "print(essem_evalute(val_dataloader,model_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = self.data.iloc[idx]['id']\n",
    "        utterance = str(self.data.iloc[idx]['utterance'])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            utterance,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'id' : ids,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "test_dataset = TestDataset(test_data, tokenizer, max_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "prdict_model_list=[]\n",
    "for i in range(15):\n",
    "    path = f'./w_weight{i}.pth'\n",
    "    bert_classifier = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=num_classes).to(device)\n",
    "    bert_classifier.load_state_dict(torch.load(path))\n",
    "    prdict_model_list.append(bert_classifier)\n",
    "    bert_classifier.eval()\n",
    "y_pred = []\n",
    "ids = []\n",
    "with torch.no_grad():\n",
    "    for batch  in test_dataloader:\n",
    "        id_0 = batch['id'].cpu().item()\n",
    "        ids.append(id_0)\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_attn_mask = batch['attention_mask'].to(device)\n",
    "        tmp = 0\n",
    "        for i in range(len(prdict_model_list)):\n",
    "            logits = prdict_model_list[i](b_input_ids, b_attn_mask)\n",
    "            \n",
    "            tmp += torch.sigmoid(logits.logits)\n",
    "        tmp = tmp / len(prdict_model_list)\n",
    "        y_pred.extend(tmp.cpu().detach().numpy().tolist())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = (np.array(y_pred) > 0.6).astype(int)\n",
    "# y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_preds, columns=tag_list)\n",
    "df_id =pd.DataFrame(ids, columns=[\"id\"])\n",
    "merged_df = pd.concat([df_id, df], axis=1)\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = merged_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"submission.csv\", 'w', newline='') as csvfile:\n",
    "    fieldnames=[\"id\",\"AM\",\"MS\",\"OTHER\",\"PH\",\"SF\",\"SR\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
