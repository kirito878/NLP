{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part1 predict gold index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujh1123/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/train.json\n",
      "Loading data from dataset/val.json\n",
      "Loading data from dataset/test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3696it [00:05, 687.43it/s]\n",
      "792it [00:01, 733.62it/s]\n",
      "792it [00:01, 627.00it/s]\n",
      "100%|██████████| 3696/3696 [02:19<00:00, 26.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.3894, Validation Loss: 0.3270, Validation Accuracy: 0.8741\n",
      "all_gold_pred_val: [2, 3, 5, 6, 7, 8, 9]\n",
      "all_gold_pred_test: [0, 1, 2, 4, 5, 7, 10, 11]\n",
      "Loading data from dataset/val.json\n",
      "Loading data from dataset/test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx][0]['input_ids'].squeeze()\n",
    "        attention_mask = self.data[idx][0]['attention_mask'].squeeze()\n",
    "        labels = self.data[idx][1]\n",
    "        return input_ids, attention_mask, labels\n",
    "    \n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx][0]['input_ids'].squeeze()\n",
    "        attention_mask = self.data[idx][0]['attention_mask'].squeeze()\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "def update_data(file_name, all_gold_pred):\n",
    "    # Load the existing data\n",
    "    data = load_data(file_name)\n",
    "\n",
    "    # Modify the data\n",
    "    for i, item in enumerate(data):\n",
    "        item['s.gold.index.predict'] = all_gold_pred[i]\n",
    "\n",
    "    updated_file_name = './dataset/updated_' + file_name\n",
    "    with open(updated_file_name, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def load_data(file_name):\n",
    "    file_path = os.path.join('dataset', file_name)\n",
    "    print(f'Loading data from {file_path}')\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.strip().lower().replace('[', '').replace(']', '')\n",
    "\n",
    "def tokenize_and_format(data, tokenizer, max_length=128):\n",
    "    processed_data = []\n",
    "\n",
    "    for i, item in tqdm(enumerate(data)):\n",
    "        utterance = preprocess_text(item['u'])\n",
    "        situational_statements = [preprocess_text(statement) for statement in item['s']]\n",
    "        gold_index = item.get('s.gold.index', [])\n",
    "        response = preprocess_text(item['r'])\n",
    "        response_label = item.get('r.label', None)\n",
    "        for index, statement in enumerate(situational_statements):\n",
    "            formatted_text = f'{utterance} {response} {statement}'\n",
    "            if index in gold_index:\n",
    "                label_tensor = torch.tensor(1, dtype=torch.float)\n",
    "            else:\n",
    "                label_tensor = torch.tensor(0, dtype=torch.float)\n",
    "\n",
    "            tokenized_text = tokenizer.encode_plus(\n",
    "                formatted_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            processed_data.append((tokenized_text, label_tensor))\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def train(model, train_data, val_data, test_data, optimizer, criterion, device, epochs):\n",
    "    model.to(device)\n",
    "\n",
    "    min_val_loss = 10000\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, labels in tqdm(train_data):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # 執行模型\n",
    "            outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "            \n",
    "            # print(f'outputs: {outputs.shape}, labels: {labels.shape}')\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(f'outputs: {outputs}, labels: {labels}')\n",
    "        # 計算驗證集的損失\n",
    "        val_loss = 0\n",
    "        accuracy = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        all_gold_pred_val = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in val_data:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.unsqueeze(1).to(device)\n",
    "\n",
    "                # print(f'input_ids: {input_ids}, attention_mask: {attention_mask}, labels: {labels}')\n",
    "                # exit(0)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "                # 計算驗證集的準確率, if accuracy > 0.5, then 1, else 0\n",
    "                # print(f'outputs: {outputs}, labels: {labels}')\n",
    "                outputs = (torch.sigmoid(outputs) > 0.5).cpu().numpy().astype(int)\n",
    "\n",
    "                gold_pred = [i for i, output in enumerate(outputs) if output == 1]\n",
    "                all_gold_pred_val.append(gold_pred)\n",
    "\n",
    "                all_val_preds.append(outputs)\n",
    "                all_val_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            all_val_preds = np.concatenate(all_val_preds, axis=0)\n",
    "            all_val_labels = np.concatenate(all_val_labels, axis=0)\n",
    "            accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "\n",
    "            if val_loss/len(val_data) < min_val_loss:\n",
    "                min_val_loss = val_loss/len(val_data)\n",
    "                # print(f'Saving model at epoch {epoch+1}, min_val_loss: {min_val_loss:.4f}')\n",
    "                torch.save(model.state_dict(), 'best_index_model.pt')\n",
    "\n",
    "        tqdm.write(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                   f'Loss: {total_loss / len(train_data):.4f}, '\n",
    "                   f'Validation Loss: {val_loss/len(val_data):.4f}, '\n",
    "                   f'Validation Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_gold_pred_test = []\n",
    "        for input_ids, attention_mask in test_data:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "            outputs = (torch.sigmoid(outputs) > 0.5).cpu().numpy().astype(int)\n",
    "\n",
    "            gold_pred = [i for i, output in enumerate(outputs) if output == 1]\n",
    "\n",
    "            all_gold_pred_test.append(gold_pred)\n",
    "    \n",
    "    return all_gold_pred_val, all_gold_pred_test\n",
    "\n",
    "def calculate_pos_weight(train_data):\n",
    "    positive_count = sum([label for _, _, label in train_data])\n",
    "    total_count = len(train_data)\n",
    "    negative_count = total_count - positive_count\n",
    "    print(f'positive_count: {positive_count}, negative_count: {negative_count}')\n",
    "    return negative_count.float() / positive_count.float()\n",
    "\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# Adjust the file paths as necessary\n",
    "train_data = load_data('train.json')\n",
    "val_data = load_data('val.json')\n",
    "test_data = load_data('test.json')\n",
    "\n",
    "processed_train = tokenize_and_format(train_data, tokenizer)\n",
    "processed_val = tokenize_and_format(val_data, tokenizer)\n",
    "processed_test = tokenize_and_format(test_data, tokenizer)\n",
    "epochs = 1\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "optimizer = Lion(model.parameters(), lr=1e-6, weight_decay=5e-2)\n",
    "\n",
    "# 轉換為 Dataset\n",
    "train_dataset = CustomDataset(processed_train)\n",
    "val_dataset = CustomDataset(processed_val)\n",
    "test_dataset = CustomTestDataset(processed_test)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 設定 batch size\n",
    "batch_size = 12  # 或者您希望的其他數值\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 創建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 呼叫訓練函數\n",
    "all_gold_pred_val, all_gold_pred_test = train(model, train_loader, val_loader, test_loader, optimizer, criterion, device, epochs)\n",
    "\n",
    "print(f'all_gold_pred_val: {all_gold_pred_val[0]}')\n",
    "print(f'all_gold_pred_test: {all_gold_pred_test[0]}')\n",
    "\n",
    "# write to json file, add new key: s.gold.index.predict\n",
    "update_data('val.json', all_gold_pred_val)\n",
    "update_data('test.json', all_gold_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part2 preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from gold import get_gold_indices\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(file_name):\n",
    "    file_path = os.path.join('dataset', file_name)\n",
    "    print(f'Loading data from {file_path}')\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.strip().lower().replace('[', '').replace(']', '')\n",
    "\n",
    "def tokenize_and_format(data, tokenizer, max_length=256, dataset_type='train'):\n",
    "    processed_data = []\n",
    "\n",
    "    for i, item in tqdm(enumerate(data)):\n",
    "        utterance = preprocess_text(item['u'])\n",
    "        situational_statements = [preprocess_text(statement) for statement in item['s']]\n",
    "        # situational_types = item['s.type']\n",
    "        response = preprocess_text(item['r'])\n",
    "        response_label = item.get('r.label', None)\n",
    "\n",
    "        if dataset_type != 'train':\n",
    "            gold_index = item.get('s.gold.index.predict', [])\n",
    "            combined_statements = ' '.join([f\"{preprocess_text(statement)}\"\n",
    "                                            for index, statement in enumerate(situational_statements) if index in gold_index])\n",
    "        else:\n",
    "            gold_index = item.get('s.gold.index', [])\n",
    "            combined_statements = ' '.join([f\"{preprocess_text(statement)}\"\n",
    "                                            for index, statement in enumerate(situational_statements) if index in gold_index])\n",
    "        \n",
    "        formatted_text = f\"{utterance} {combined_statements} {response}\"\n",
    "        # formatted_text = f\"{utterance}  {response}\"\n",
    "\n",
    "        # BERT tokenizer\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            formatted_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "\n",
    "        if dataset_type == 'test':\n",
    "            label_tensor = None\n",
    "            processed_data.append(tokenized_text)\n",
    "        else:\n",
    "            label_tensor = torch.tensor(response_label, dtype=torch.float) if response_label is not None else None\n",
    "            processed_data.append((tokenized_text, label_tensor))\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part3 train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/wujh1123/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/train.json\n",
      "Loading data from dataset/updated_val.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3696it [00:00, 5285.17it/s]\n",
      "792it [00:00, 5409.28it/s]\n",
      "100%|██████████| 462/462 [01:26<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Loss: 0.7474, Validation Loss: 0.8692, Validation Accuracy: 0.7323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [01:26<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Loss: 0.3767, Validation Loss: 0.8438, Validation Accuracy: 0.7955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [01:26<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Loss: 0.2055, Validation Loss: 1.0434, Validation Accuracy: 0.7942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [01:26<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Loss: 0.1435, Validation Loss: 1.3464, Validation Accuracy: 0.8005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [01:26<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Loss: 0.0995, Validation Loss: 1.3225, Validation Accuracy: 0.8119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [01:26<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Loss: 0.0676, Validation Loss: 1.5578, Validation Accuracy: 0.7917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [01:26<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Loss: 0.0557, Validation Loss: 1.6171, Validation Accuracy: 0.7955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [01:26<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Loss: 0.0602, Validation Loss: 1.9707, Validation Accuracy: 0.7955\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from preprocess import load_data, tokenize_and_format\n",
    "from lion_pytorch import Lion\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx][0]['input_ids'].squeeze()\n",
    "        attention_mask = self.data[idx][0]['attention_mask'].squeeze()\n",
    "        labels = self.data[idx][1]\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "def train(model, train_data, val_data, optimizer, criterion, device, epochs):\n",
    "    model.to(device)\n",
    "\n",
    "    min_val_loss = 10000\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, labels in tqdm(train_data):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # 執行模型\n",
    "            outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "            \n",
    "            # print(f'outputs: {outputs.shape}, labels: {labels.shape}')\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(f'outputs: {outputs}, labels: {labels}')\n",
    "        # 計算驗證集的損失\n",
    "        val_loss = 0\n",
    "        accuracy = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in val_data:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.unsqueeze(1).to(device)\n",
    "\n",
    "                # print(f'input_ids: {input_ids}, attention_mask: {attention_mask}, labels: {labels}')\n",
    "                # exit(0)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "                # 計算驗證集的準確率, if accuracy > 0.5, then 1, else 0\n",
    "                # print(f'outputs: {outputs}, labels: {labels}')\n",
    "                outputs = (outputs > 0.5).cpu().numpy().astype(float)\n",
    "\n",
    "                all_val_preds.append(outputs)\n",
    "                all_val_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            all_val_preds = np.concatenate(all_val_preds, axis=0)\n",
    "            all_val_labels = np.concatenate(all_val_labels, axis=0)\n",
    "            # print(f'all_val_preds: {all_val_preds[:5]}, all_val_labels: {all_val_labels[:5]}')\n",
    "            # print(f'all_val_preds: {np.sum(all_val_preds, axis=0)}, all_val_labels: {np.sum(all_val_labels, axis=0)}')\n",
    "            accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "\n",
    "            if val_loss/len(val_data) < min_val_loss:\n",
    "                min_val_loss = val_loss/len(val_data)\n",
    "                # print(f'Saving model at epoch {epoch+1}, min_val_loss: {min_val_loss:.4f}')\n",
    "                torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "        tqdm.write(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                   f'Loss: {total_loss / len(train_data):.4f}, '\n",
    "                   f'Validation Loss: {val_loss/len(val_data):.4f}, '\n",
    "                   f'Validation Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "\n",
    "\n",
    "def calculate_pos_weight(train_data):\n",
    "    positive_count = sum([label for _, _, label in train_data])\n",
    "    total_count = len(train_data)\n",
    "    negative_count = total_count - positive_count\n",
    "    return negative_count.float() / positive_count.float()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'microsoft/deberta-v3-large'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_data = load_data('train.json')\n",
    "val_data = load_data('updated_val.json')\n",
    "\n",
    "processed_train = tokenize_and_format(train_data, tokenizer, dataset_type='train')\n",
    "processed_val = tokenize_and_format(val_data, tokenizer, dataset_type='val')\n",
    "\n",
    "epochs = 8\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "optimizer = Lion(model.parameters(), lr=1e-6, weight_decay=5e-2)\n",
    "\n",
    "\n",
    "# 轉換為 Dataset\n",
    "train_dataset = CustomDataset(processed_train)\n",
    "val_dataset = CustomDataset(processed_val)\n",
    "\n",
    "pos_weight = calculate_pos_weight(train_dataset)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "\n",
    "# 設定 batch size\n",
    "batch_size = 8  # 或者您希望的其他數值\n",
    "\n",
    "# 創建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 呼叫訓練函數\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part4 inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujh1123/anaconda3/envs/nlp/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/updated_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "792it [00:00, 4973.91it/s]\n",
      "100%|██████████| 99/99 [00:05<00:00, 18.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from preprocess import load_data, tokenize_and_format\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx]['input_ids'].squeeze()\n",
    "        attention_mask = self.data[idx]['attention_mask'].squeeze()\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "def predict(model, test_data, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(test_data):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).cpu().numpy().astype(int)\n",
    "            predictions.append(preds)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    return predictions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 加载并处理测试数据\n",
    "test_data = load_data('updated_test.json')\n",
    "processed_test = tokenize_and_format(test_data, tokenizer, dataset_type='test')\n",
    "\n",
    "# 转换为 Dataset\n",
    "test_dataset = CustomDataset(processed_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# 进行预测\n",
    "test_preds = predict(model, test_loader, device)\n",
    "\n",
    "# 创建 submission 文件\n",
    "submission = pd.DataFrame(test_preds, columns=['response_quality'])\n",
    "submission.to_csv('submission.csv', index=True, index_label='index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part5 ensemble part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the five CSV files\n",
    "df1 = pd.read_csv('data/responses.csv')\n",
    "df2 = pd.read_csv('data/responses.csv')\n",
    "df3 = pd.read_csv('data/submission_85123.csv')\n",
    "df4 = pd.read_csv('data/submission_85123.csv')  # replace with your actual filename\n",
    "df5 = pd.read_csv('data/submission_result.csv')  # replace with your actual filename\n",
    "\n",
    "# Set 'index' as the index for correct row-wise operations\n",
    "df1.set_index('index', inplace=True)\n",
    "df2.set_index('index', inplace=True)\n",
    "df3.set_index('index', inplace=True)\n",
    "df4.set_index('index', inplace=True)\n",
    "df5.set_index('index', inplace=True)\n",
    "\n",
    "# Sum the response_quality columns\n",
    "sum_df = df1['response_quality'] + df2['response_quality'] + df3['response_quality'] + df4['response_quality'] + df5['response_quality']\n",
    "\n",
    "# Create a new DataFrame for the result\n",
    "result_df = pd.DataFrame(sum_df, columns=['response_quality'])\n",
    "\n",
    "# If the sum is greater than or equal to 3, set to 1; otherwise, set to 0\n",
    "result_df['response_quality'] = result_df['response_quality'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "\n",
    "# Reset the index to turn it back into a column\n",
    "result_df.reset_index(inplace=True)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('data/submission_result2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
